{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc0786a7",
   "metadata": {},
   "source": [
    "### Experiment 1: agent A generates ground truth (GT)\n",
    "\n",
    "In this asymmetric two-model configuration, we evaluate how well Agent B aligns with Agent A's intent, treating A's generated query and description as the ground truth (GT). This setup embodies the Recursive Consciousness (RC) framework's principle of a forgetful subsystem (B) reconstructing \"axioms\" (A's intent) through recursive probing, with agent O mediating questions to refine understanding toward an epistemic fixpoint (minimal semantic divergence).\n",
    "\n",
    "Agent A generates a query $Q$ and its ground truth understanding $s_{\\text{GT}}$, while Agent B interprets $Q$ to produce its understanding. Agent O generates a set of N questions (potentially dependent on $Q$) and sends them to both agents. Responses from B are used to iteratively refine its alignment to $s_{\\text{GT}}$, with computation focusing on B's convergence. Querying A maintains symmetry for consistency checks or role-swaps but is optional for the core evaluation, as A's responses reinforce the fixed GT. The goal is to measure B's alignment to A's GT via a simplified understanding equation, quantifying interpretive fidelity in line with RC's lossy translations ($I \\dashv M$).\n",
    "\n",
    "#### Workflow\n",
    "\n",
    "* **A Generates GT:** Agent A produces $Q$ and $s_{\\text{GT}}$ (extended description of intent).\n",
    "* **B Interprets Q:** Agent B generates its initial understanding of $Q$.\n",
    "* **O Generates Questions:** O creates N adaptive questions based on $Q$ (e.g., intent clarification, accuracy probes).\n",
    "* **Query Both Agents:** Send questions to A and B; append responses to their respective histories.\n",
    "* **Iterative Refinement and Termination:** For each iteration (starting from 2), compute U on current understandings; terminate early if $\\Delta U < \\epsilon$ (minimal change) or at max N.\n",
    "* **Compute Understanding:** Use B's responses vs A's GT for the simplified U(AB); optionally compute for symmetry.\n",
    "* **Role-Swap:** Repeat with B as GT generator and A as interpreter for U(BA), highlighting non-commutativity ($\\Phi_{A\\to B} \\neq \\Phi_{B\\to A}$).\n",
    "\n",
    "#### RC Understanding Equation $U_{\\text{mutual}}$\n",
    "\n",
    "The mutual understanding metric quantifies the interpretive fidelity between two agents' understandings, incorporating semantic distance and recency-weighted contributions from their histories. The equation is defined as follows:\n",
    "\n",
    "$$\n",
    "U_{\\text{mutual}}(s_A, s_B) = \\sum_{n=1}^N w_n \\left[ (1 - D_{\\text{JS}}(P^{(n)}_A \\| P^{(n)}_B)) \\cdot (1 - d^{(n)}_{\\text{sem}}) \\cdot \\sqrt{\\kappa^{(n)}_A \\kappa_B^{(n)}} \\right]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "weights: $w_n = \\frac{2n}{N(N+1)}$ (linear recency, summing to 1);\n",
    "\n",
    "$$\n",
    "D_{\\text{JS}}(P_A \\| P_B) = \\frac{1}{2} \\left[ D_{\\text{KL}}(P_A \\| M) + D_{\\text{KL}}(P_B \\| M) \\right]\n",
    "$$\n",
    "where $M = \\frac{1}{2}(P_A + P_B)$;\n",
    "\n",
    "$$\n",
    "d_{\\text{sem}}(s_A, s_B) = \\frac{1 - \\cos(E(s_A), E(s_B))}{2} = \\sin^2(\\theta/2)\n",
    "$$\n",
    "\n",
    "where $\\theta = \\arccos(\\langle \\hat{E}(s_A), \\hat{E}(s_B) \\rangle)$;\n",
    "\n",
    "$$\n",
    "\\kappa^{(n)}_{A/B} = \\exp\\left( - \\|s^{(n)}_{A/B} - s^{(n-1)}_{A/B}\\| \\right)\n",
    "$$\n",
    "\n",
    "with the geometric mean $\\sqrt{\\kappa_A^{(n)} \\kappa_B^{(n)}}$ symmetrizing loops and bounding the Lipschitz constant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e023332d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e62b6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "# Set to None to use local AI models, OpenAI API key is still required but will be used ony for embeddings model\n",
    "# OPENAI_API_KEY = None\n",
    "\n",
    "LOCAL_AI_API_URL = \"http://localhost:8080/v1\"\n",
    "\n",
    "LLM_MODEL_A1 = \"gpt-4.1-mini\"\n",
    "LLM_MODEL_A2 = \"o1\"\n",
    "LLM_MODEL_A3 = \"o4-mini\"\n",
    "LLM_MODEL_A4 = \"o3-mini\"\n",
    "LLM_MODEL_A5 = \"o3\"\n",
    "LLM_MODEL_A6 = \"gpt-4.1\"\n",
    "LLM_MODEL_LOCAL = \"local\"\n",
    "\n",
    "EMBEDDING_MODEL = \"text-embedding-3-large\"\n",
    "\n",
    "MAX_ITERATIONS = 10\n",
    "MIN_ITERATIONS = 5\n",
    "EPSILON = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f77ca3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed using OpenAI API\n",
    "def embed(text: str) -> np.ndarray:\n",
    "    try:\n",
    "        response = openai.embeddings.create(\n",
    "            model=EMBEDDING_MODEL,\n",
    "            input=text\n",
    "        )\n",
    "        return np.array(response.data[0].embedding)\n",
    "    except Exception as e:\n",
    "        print(f\"Error embedding text: {e}\")\n",
    "        return np.array([])  # Return empty array on error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5d6864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat with error handling\n",
    "def chat(model: str, messages: List[dict], temperature: float = 0.7) -> str:\n",
    "    try:\n",
    "        response = openai.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages\n",
    "            # temperature=temperature,\n",
    "            # max_tokens=200\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"API error: {e}\")\n",
    "        time.sleep(5)  # Rate limit backoff\n",
    "        return \"\"  # Fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba52eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_local(model: str, messages: List[dict], base_url: str = LOCAL_AI_API_URL, temperature: float = 0.7) -> str:\n",
    "    try:\n",
    "        client = openai.Client(base_url=base_url)\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            max_tokens=300\n",
    "            # temperature=temperature\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Local API error: {e}\")\n",
    "        time.sleep(5)  # Rate limit backoff\n",
    "        return \"\"  # Fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53169977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(emb1: np.ndarray, emb2: np.ndarray) -> float:\n",
    "    # Handle empty arrays (error case)\n",
    "    if emb1.size == 0 or emb2.size == 0 or emb1.shape != emb2.shape:\n",
    "        raise ValueError(\"Embeddings must be non-empty and of the same shape in cosine_sim\")\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    norm1 = np.linalg.norm(emb1)\n",
    "    norm2 = np.linalg.norm(emb2)\n",
    "\n",
    "    # Handle zero vectors\n",
    "    if norm1 == 0 or norm2 == 0:\n",
    "        return 1.0\n",
    "\n",
    "    cos = np.dot(emb1, emb2) / (norm1 * norm2)\n",
    "    return cos\n",
    "\n",
    "# Helper function for softmax with better numerical stability\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for array x with enhanced numerical stability.\"\"\"\n",
    "    if len(x) == 0:\n",
    "        return np.array([])\n",
    "\n",
    "    # Enhanced numerical stability\n",
    "    x_shifted = x - np.max(x)\n",
    "    exp_x = np.exp(np.clip(x_shifted, -500, 500))  # Prevent overflow/underflow\n",
    "    sum_exp = np.sum(exp_x)\n",
    "\n",
    "    if sum_exp == 0 or not np.isfinite(sum_exp):\n",
    "        # Fallback to uniform distribution\n",
    "        return np.ones(len(x)) / len(x)\n",
    "\n",
    "    return exp_x / sum_exp\n",
    "\n",
    "# Improved Mutual Understanding with better error handling and interpretation\n",
    "def mutual_understanding(single_emb, multi_emb):\n",
    "    \"\"\"Calculate Jensen-Shannon divergence as proxy for mutual understanding.\"\"\"\n",
    "\n",
    "    if single_emb.size == 0 or multi_emb.size == 0 or single_emb.shape != multi_emb.shape:\n",
    "        raise ValueError(\"Embeddings must be non-empty and have matching shapes in mutual_understanding.\")\n",
    "\n",
    "    try:\n",
    "        # Normalize embeddings to same scale\n",
    "        single_norm = single_emb / (np.linalg.norm(single_emb) + 1e-10)\n",
    "        multi_norm = multi_emb / (np.linalg.norm(multi_emb) + 1e-10)\n",
    "\n",
    "        # Create pseudo-distributions via softmax\n",
    "        p_s = softmax(single_norm * 10)  # Scale for better separation\n",
    "        p_m = softmax(multi_norm * 10)\n",
    "\n",
    "        # Compute Jensen-Shannon divergence with smoothing\n",
    "        epsilon = 1e-10\n",
    "        m = 0.5 * (p_s + p_m)\n",
    "        kl_sm = np.sum(p_s * np.log((p_s + epsilon) / (m + epsilon)))\n",
    "        kl_ms = np.sum(p_m * np.log((p_m + epsilon) / (m + epsilon)))\n",
    "        divergence = 0.5 * (kl_sm + kl_ms)\n",
    "        return max(0, divergence)  # Ensure non-negative\n",
    "\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error in mutual_understanding calculation: {e}.\")\n",
    "\n",
    "def semantic_distance(emb1: np.ndarray, emb2: np.ndarray) -> float:\n",
    "    cos = cosine_sim(emb1, emb2)\n",
    "    # Compute d_sem = (1 - cos) / 2 = sin^2(theta/2)\n",
    "    return (1 - cos) / 2\n",
    "\n",
    "def kappa(dist_A: float, dist_B: float) -> float:\n",
    "    return np.exp(-(dist_A + dist_B) / 2)\n",
    "\n",
    "def mutual_understanding_advanced(history_a: List[np.ndarray], history_b: List[np.ndarray]) -> float:\n",
    "    if len(history_a) == 0 or len(history_b) == 0 or len(history_a) != len(history_b):\n",
    "        raise ValueError(\"History lists must be of the same length and non-empty in mutual_understanding_advanced.\")\n",
    "\n",
    "    total = 0.0\n",
    "    N = len(history_a)  # Number of iterations\n",
    "    denom = N * (N + 1)\n",
    "    for n in range(1, N + 1):  # n from 1 to N\n",
    "        i = n - 1  # 0-based index\n",
    "        if history_a[i].size == 0 or history_b[i].size == 0 or history_a[i].shape != history_b[i].shape:\n",
    "            raise ValueError(\"History entries must be non-empty in mutual_understanding_advanced.\")\n",
    "\n",
    "        divergence = mutual_understanding(history_a[i], history_b[i])\n",
    "        align_prob = 1 - divergence\n",
    "        d_sem = semantic_distance(history_a[i], history_b[i])\n",
    "        align_sem = 1 - d_sem\n",
    "        if n > 1:\n",
    "            dist_prev_A = semantic_distance(history_a[i], history_a[i-1])\n",
    "            dist_prev_B = semantic_distance(history_b[i], history_b[i-1])\n",
    "            k = kappa(dist_prev_A, dist_prev_B)\n",
    "        else:\n",
    "            k = 1.0  # For n=1: No previous iteration, set stability factor to 1.0\n",
    "        w_n = (2 * n) / denom\n",
    "        total += w_n * (align_prob * align_sem * k)\n",
    "\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69352798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gt_prompt(topic: str = None) -> str:\n",
    "    \"\"\"Create the ground truth generation prompt.\"\"\"\n",
    "    base_prompt = \"Generate a complex, thought-provoking question and provide your detailed understanding of what this question is asking for. Format your response as:\\nQUESTION: [your question]\\nUNDERSTANDING: [detailed explanation of the question's intent, context, and what constitutes a good answer]\"\n",
    "\n",
    "    if topic:\n",
    "        return f\"Generate a complex question about {topic} and provide your detailed understanding of what this question is asking for. Format your response as:\\nQUESTION: [your question]\\nUNDERSTANDING: [detailed explanation of the question's intent, context, and what constitutes a good answer]\"\n",
    "\n",
    "    return base_prompt\n",
    "\n",
    "def create_o_prompt(\n",
    "    s_GT: str,\n",
    "    Q: str,\n",
    "    N: int = MAX_ITERATIONS\n",
    ") -> str:\n",
    "    \"\"\"Create the facilitator prompt for generating probing questions.\"\"\"\n",
    "\n",
    "    o_prompt = f\"\"\"Based on this question: \"{Q}\",\n",
    "\n",
    "and its initial understanding by the entity posing this question: \"{s_GT}\"\n",
    "\n",
    "Generate {N} unique probing questions that would help reveal the intent and understanding of the question.\n",
    "Focus on areas where the interpretations might differ or need clarification.\n",
    "Your output must only include {N} questions, one question per line, nothing else.\n",
    "\"\"\"\n",
    "    return o_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1803dd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_iterative_questioning_phase(\n",
    "    gt_agent_model: str,\n",
    "    interpreter_model: str,\n",
    "    orbiter_model: str,\n",
    "    Q: str,\n",
    "    s_GT: str,\n",
    "    gt_agent_name: str,\n",
    "    interpreter_name: str,\n",
    "    max_iterations: int,\n",
    "    epsilon: float\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Run one phase of the iterative questioning process.\n",
    "\n",
    "    Args:\n",
    "        gt_agent_model: Model for the GT-generating agent\n",
    "        interpreter_model: Model for the interpreting agent\n",
    "        orbiter_model: Model for the orbiter (question generator)\n",
    "        Q: The initial question\n",
    "        s_GT: The GT agent's understanding\n",
    "        gt_agent_name: Name of GT agent (\"A\" or \"B\")\n",
    "        interpreter_name: Name of interpreter agent (\"B\" or \"A\")\n",
    "        max_iterations: Maximum iterations\n",
    "        epsilon: Convergence threshold\n",
    "        topic: Optional topic for context\n",
    "\n",
    "    Returns:\n",
    "        dict: Phase results with understanding, iterations, and conversation history\n",
    "    \"\"\"\n",
    "    print(f\"\\n🔄 PHASE: Agent {gt_agent_name} as Ground Truth Generator\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    delta_U = 0.0\n",
    "\n",
    "    # Step 1: Initial setup\n",
    "    interpreter_initial_prompt = f\"Question: {Q}\\n\\nProvide your understanding of what this question is asking for, including context, intent, and what would constitute a good answer. Do not say anything else.\"\n",
    "    if interpreter_model == LLM_MODEL_LOCAL:\n",
    "        interpreter_initial_response = chat_local(interpreter_model, [{\"role\": \"user\", \"content\": interpreter_initial_prompt}])\n",
    "    else:\n",
    "        interpreter_initial_response = chat(interpreter_model, [{\"role\": \"user\", \"content\": interpreter_initial_prompt}])\n",
    "    print(f\"\\n🤖 Agent {interpreter_name}'s initial interpretation:\")\n",
    "    print(f\"   {interpreter_initial_response[:100]}...\")\n",
    "\n",
    "    # Initialize conversation histories\n",
    "    history_gt = [{\"role\": \"user\", \"content\": Q}, {\"role\": \"assistant\", \"content\": s_GT}]\n",
    "    history_interpreter = [{\"role\": \"user\", \"content\": Q}, {\"role\": \"assistant\", \"content\": interpreter_initial_response}]\n",
    "\n",
    "    # Store embeddings for iterations - separate tracking for question responses vs understanding\n",
    "    emb_gt = embed(s_GT)  # GT agent's understanding\n",
    "    emb_interpreter = embed(interpreter_initial_response)\n",
    "    iter_emb_gt_responses = [emb_gt]  # GT agent's responses to questions\n",
    "    iter_emb_interpreter_responses = [iter_emb_gt_responses]  # Interpreter's responses to questions\n",
    "\n",
    "    # Track understanding evolution separately\n",
    "    iter_emb_gt_understanding = [emb_gt]  # GT agent's understanding evolution\n",
    "    iter_emb_interpreter_understanding = [emb_interpreter]  # Interpreter's understanding evolution\n",
    "\n",
    "    conversation_history = [{\n",
    "        'iteration': 0,\n",
    "        f'{gt_agent_name}_understanding': s_GT,\n",
    "        f'{interpreter_name}_understanding': interpreter_initial_response,\n",
    "        'question': Q\n",
    "    }]\n",
    "\n",
    "    # Iterative refinement\n",
    "    prev_U = 0.0\n",
    "    o_prompt = create_o_prompt(\n",
    "        s_GT=s_GT,\n",
    "        Q=Q,\n",
    "        N=max_iterations\n",
    "    )\n",
    "    questions = chat(orbiter_model, [{\"role\": \"user\", \"content\": o_prompt}]).strip()\n",
    "    questions = questions.splitlines()\n",
    "    # delete any empty questions\n",
    "    questions = [q.strip() for q in questions if q.strip() != \"\"]\n",
    "    print(f\"❓ Orbiter generated {len(questions)} questions.\")\n",
    "    max_iterations = min(max_iterations, len(questions))\n",
    "\n",
    "    for iteration in range(1, max_iterations + 1):\n",
    "        print(f\"\\n🔄 Iteration {iteration}\")\n",
    "\n",
    "        # O Generates One Question per iteration\n",
    "        question = questions[iteration - 1]\n",
    "        # Query both agents with the single question\n",
    "        iteration_data = {\n",
    "            'iteration': iteration,\n",
    "            'question': question,\n",
    "            'U': 0.0,  # Placeholder for understanding value\n",
    "            f'{gt_agent_name}_response': \"\",\n",
    "            f'{interpreter_name}_response': \"\",\n",
    "            f'{gt_agent_name}_understanding': \"\",\n",
    "            f'{interpreter_name}_understanding': \"\",\n",
    "        }\n",
    "\n",
    "        # Query GT agent - their response to this specific question\n",
    "        history_gt.append({\"role\": \"user\", \"content\": question})\n",
    "        if gt_agent_model == LLM_MODEL_LOCAL:\n",
    "            gt_response_to_question = chat_local(gt_agent_model, history_gt)\n",
    "        else:\n",
    "            gt_response_to_question = chat(gt_agent_model, history_gt)\n",
    "        history_gt.append({\"role\": \"assistant\", \"content\": gt_response_to_question})\n",
    "        iteration_data[f'{gt_agent_name}_response'] = gt_response_to_question\n",
    "\n",
    "        # Query interpreter - their response to the same specific question\n",
    "        history_interpreter.append({\"role\": \"user\", \"content\": question})\n",
    "        if interpreter_model == LLM_MODEL_LOCAL:\n",
    "            interpreter_response_to_question = chat_local(interpreter_model, history_interpreter)\n",
    "        else:\n",
    "            interpreter_response_to_question = chat(interpreter_model, history_interpreter)\n",
    "        history_interpreter.append({\"role\": \"assistant\", \"content\": interpreter_response_to_question})\n",
    "        iteration_data[f'{interpreter_name}_response'] = interpreter_response_to_question\n",
    "\n",
    "        print(f\"   {gt_agent_name}: {gt_response_to_question[:80]}...\")\n",
    "        print(f\"   {interpreter_name}: {interpreter_response_to_question[:80]}...\")\n",
    "\n",
    "        # Store embeddings of responses to the SAME question\n",
    "        iter_emb_gt_responses.append(embed(gt_response_to_question))\n",
    "        iter_emb_interpreter_responses.append(embed(interpreter_response_to_question))\n",
    "\n",
    "        prompt = f\"Based on the above dialog and discussing question:\\n{Q}\\n\\nSummarize your current understanding of '{Q}'.\\n\\nYour response must include only the summary of understanding, nothing else.\"\n",
    "\n",
    "        # Update understanding summaries (for stability tracking)\n",
    "        history_gt.append({\"role\": \"user\", \"content\": prompt})\n",
    "        if gt_agent_model == LLM_MODEL_LOCAL:\n",
    "            gt_updated_response = chat_local(gt_agent_model, history_gt)\n",
    "        else:\n",
    "            gt_updated_response = chat(gt_agent_model, history_gt)\n",
    "        history_gt.append({\"role\": \"assistant\", \"content\": gt_updated_response})\n",
    "        iteration_data[f'{gt_agent_name}_understanding'] = gt_updated_response\n",
    "\n",
    "        history_interpreter.append({\"role\": \"user\", \"content\": prompt})\n",
    "        if interpreter_model == LLM_MODEL_LOCAL:\n",
    "            interpreter_updated_response = chat_local(interpreter_model, history_interpreter)\n",
    "        else:\n",
    "            interpreter_updated_response = chat(interpreter_model, history_interpreter)\n",
    "        history_interpreter.append({\"role\": \"assistant\", \"content\": interpreter_updated_response})\n",
    "        iteration_data[f'{interpreter_name}_understanding'] = interpreter_updated_response\n",
    "\n",
    "        # Store embeddings of understanding evolution (for stability calculation)\n",
    "        iter_emb_gt_understanding.append(embed(gt_updated_response))\n",
    "        iter_emb_interpreter_understanding.append(embed(interpreter_updated_response))\n",
    "\n",
    "        # Compute U (simplified version) - comparing responses to the same questions\n",
    "        current_U = 0.0\n",
    "        if iteration >= 1:\n",
    "            current_U = mutual_understanding_advanced(\n",
    "                iter_emb_gt_understanding,\n",
    "                iter_emb_interpreter_understanding\n",
    "            )\n",
    "            iteration_data['U'] = current_U\n",
    "            print(f\"   💯 U_simplified at iteration {iteration}: {current_U:.4f}\")\n",
    "\n",
    "        conversation_history.append(iteration_data)\n",
    "\n",
    "        # Check for convergence (only after min MIN_ITERATIONS)\n",
    "        if prev_U > 0:\n",
    "            d = abs(current_U - prev_U)\n",
    "            print(f\"   📈 ΔU: {d:.6f} (threshold: {epsilon})\")\n",
    "            if iteration >= MIN_ITERATIONS and d  < epsilon and delta_U < epsilon:\n",
    "                print(f\"   ✅ Converged at iteration {iteration}\")\n",
    "                break\n",
    "            delta_U = d\n",
    "        prev_U = current_U\n",
    "\n",
    "    final_U = current_U\n",
    "    print(f\"\\n📊 Phase Results: U_{gt_agent_name}→{interpreter_name} = {final_U:.4f} in {iteration} iterations\")\n",
    "\n",
    "    return {\n",
    "        'U': final_U,\n",
    "        'iterations': iteration,\n",
    "        'conversation_history': conversation_history\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc217549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gt_and_extract(model: str, topic: str = None) -> tuple[str, str, str]:\n",
    "    \"\"\"\n",
    "    Generate GT response and extract Q and s_GT.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (full_response, Q, s_GT)\n",
    "    \"\"\"\n",
    "    gt_prompt = create_gt_prompt(topic)\n",
    "    if model == LLM_MODEL_LOCAL:\n",
    "        gt_response = chat_local(model, [{\"role\": \"user\", \"content\": gt_prompt}])\n",
    "    else:\n",
    "        gt_response = chat(model, [{\"role\": \"user\", \"content\": gt_prompt}])\n",
    "    print(f\"   {gt_response[:100]}...\")\n",
    "\n",
    "    # Extract Q and s_GT\n",
    "    lines = gt_response.split('\\n')\n",
    "    Q = \"\"\n",
    "    s_GT = \"\"\n",
    "    for line in lines:\n",
    "        if line.startswith(\"QUESTION:\"):\n",
    "            Q = line.replace(\"QUESTION:\", \"\").strip()\n",
    "        elif line.startswith(\"UNDERSTANDING:\"):\n",
    "            s_GT = line.replace(\"UNDERSTANDING:\", \"\").strip()\n",
    "\n",
    "    if not Q or not s_GT:\n",
    "        Q = gt_response\n",
    "        s_GT = gt_response\n",
    "\n",
    "    return gt_response, Q, s_GT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc73b5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_workflow(model_A: str, model_B: str, model_O: str, topic: str = None,\n",
    "                       max_iterations: int = MAX_ITERATIONS, epsilon: float = EPSILON) -> dict:\n",
    "    \"\"\"\n",
    "    Complete RC Understanding Equation workflow implementation.\n",
    "\n",
    "    Args:\n",
    "        model_A: Model name for Agent A\n",
    "        model_B: Model name for Agent B\n",
    "        model_O: Model name for Orbiter O (question generator)\n",
    "        topic: Optional topic to focus the experiment on\n",
    "        max_iterations: Maximum number of iterations\n",
    "        epsilon: Convergence threshold for early termination\n",
    "\n",
    "    Returns:\n",
    "        dict: Results containing understanding, iterations, and conversation histories\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"RECURSIVE CONSCIOUSNESS UNDERSTANDING EXPERIMENT\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Phase 1: A generates GT, B interprets\n",
    "    print(\"\\n📝 Agent A generates question and GT understanding...\")\n",
    "\n",
    "    gt_response_a, Q_a, s_GT_a = generate_gt_and_extract(model_A, topic)\n",
    "    print(f\"   Q: {Q_a}\")\n",
    "    print(f\"   s_GT: {s_GT_a[:100]}...\")\n",
    "\n",
    "    # Run Phase 1\n",
    "    phase1_results = run_iterative_questioning_phase(\n",
    "        gt_agent_model=model_A,\n",
    "        interpreter_model=model_B,\n",
    "        orbiter_model=model_O,\n",
    "        Q=Q_a,\n",
    "        s_GT=s_GT_a,\n",
    "        gt_agent_name=\"A\",\n",
    "        interpreter_name=\"B\",\n",
    "        max_iterations=max_iterations,\n",
    "        epsilon=epsilon\n",
    "    )\n",
    "\n",
    "    # Phase 2: B generates GT, A interprets\n",
    "    print(\"\\n📝 Agent B generates question and GT understanding...\")\n",
    "\n",
    "    gt_response_b, Q_b, s_GT_b = generate_gt_and_extract(model_B, topic)\n",
    "\n",
    "    # Run Phase 2\n",
    "    phase2_results = run_iterative_questioning_phase(\n",
    "        gt_agent_model=model_B,\n",
    "        interpreter_model=model_A,\n",
    "        orbiter_model=model_O,\n",
    "        Q=Q_b,\n",
    "        s_GT=s_GT_b,\n",
    "        gt_agent_name=\"B\",\n",
    "        interpreter_name=\"A\",\n",
    "        max_iterations=max_iterations,\n",
    "        epsilon=epsilon\n",
    "    )\n",
    "\n",
    "    # Compile final results\n",
    "    results = {\n",
    "        'Model A': model_A,\n",
    "        'Model B': model_B,\n",
    "        'Model O': model_O,\n",
    "        'topic': topic,\n",
    "        'gt_response_A': gt_response_a,  # Store full response for debugging\n",
    "        'Q_A': Q_a,\n",
    "        's_GT_A': s_GT_a,\n",
    "        'gt_response_B': gt_response_b,  # Store full response for debugging\n",
    "        'Q_B': Q_b,\n",
    "        's_GT_B': s_GT_b,\n",
    "        'U_AB': phase1_results['U'],\n",
    "        'U_BA': phase2_results['U'],\n",
    "        'iterations_AB': phase1_results['iterations'],\n",
    "        'iterations_BA': phase2_results['iterations'],\n",
    "        'conversation_history_AB': phase1_results['conversation_history'],\n",
    "        'conversation_history_BA': phase2_results['conversation_history']\n",
    "    }\n",
    "\n",
    "    # Final summary\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"EXPERIMENT SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"🔄 U(A→B): {results['U_AB']:.4f} ({results['iterations_AB']} iterations)\")\n",
    "    print(f\"🔄 U(B→A): {results['U_BA']:.4f} ({results['iterations_BA']} iterations)\")\n",
    "    print(f\"🔄 Non-commutativity: |U(A→B) - U(B→A)| = {abs(results['U_AB'] - results['U_BA']):.4f}\")\n",
    "\n",
    "    if abs(results['U_AB'] - results['U_BA']) > epsilon:\n",
    "        print(\"✨ Significant non-commutativity detected! Φ_{A→B} ≠ Φ_{B→A}\")\n",
    "    else:\n",
    "        print(\"🤝 Approximate commutativity observed\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a52411",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_default_experiment(topic: str = None,\n",
    "                           gt_model: str = LLM_MODEL_A1,\n",
    "                           interpreter_model: str = LLM_MODEL_A3) -> dict:\n",
    "    \"\"\"\n",
    "    Run the RC Understanding Equation experiment with default models.\n",
    "\n",
    "    Args:\n",
    "        topic: Optional topic to focus the experiment on\n",
    "\n",
    "    Returns:\n",
    "        dict: Results from the experiment\n",
    "    \"\"\"\n",
    "    return experiment_workflow(\n",
    "        model_A=gt_model,                   # gpt-4.1-mini\n",
    "        model_B=interpreter_model,          # o4-mini\n",
    "        model_O=LLM_MODEL_A6,               # gpt-4.1 facilitator generates probing questions\n",
    "        topic=topic,\n",
    "        max_iterations=MAX_ITERATIONS,\n",
    "        epsilon=EPSILON                     # Proper convergence threshold\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744b4a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_experiment_results(results: dict, save_path: str = None, show_plots: bool = False):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualization of RC Understanding Equation results.\n",
    "\n",
    "    Args:\n",
    "        results: Dictionary containing experiment results\n",
    "        save_path: Optional path to save the plots\n",
    "        show_plots: Whether to display the plot (default False to prevent duplicate showing)\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(18, 12))\n",
    "\n",
    "    # Create a 2x3 grid layout\n",
    "    gs = fig.add_gridspec(2, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "    # 1. Basic Results Bar Chart\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    categories = ['V(A→B)', 'V(B→A)', 'Iterations AB', 'Iterations BA']\n",
    "    values = [results['U_AB'], results['U_BA'], results['iterations_AB'], results['iterations_BA']]\n",
    "    colors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D']\n",
    "    bars = ax1.bar(categories, values, color=colors, alpha=0.7)\n",
    "    ax1.set_title('Understanding Degrees and Iterations', fontweight='bold', fontsize=12)\n",
    "    ax1.set_ylabel('Value', fontweight='bold')\n",
    "\n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, values):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{value:.3f}' if value < 10 else f'{int(value)}',\n",
    "                ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "    # 2. Convergence Plot\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    v_ab_values = [conv['U'] for conv in results['conversation_history_AB'] if 'U' in conv and conv['U'] > 0]\n",
    "    v_ba_values = [conv['U'] for conv in results['conversation_history_BA'] if 'U' in conv and conv['U'] > 0]\n",
    "\n",
    "    if v_ab_values:\n",
    "        ax2.plot(range(1, len(v_ab_values) + 1), v_ab_values, 'o-',\n",
    "                linewidth=3, markersize=8, label='V(A→B)', color='#2E86AB', alpha=0.8)\n",
    "    if v_ba_values:\n",
    "        ax2.plot(range(1, len(v_ba_values) + 1), v_ba_values, 's-',\n",
    "                linewidth=3, markersize=8, label='V(B→A)', color='#A23B72', alpha=0.8)\n",
    "\n",
    "    ax2.set_xlabel('Iteration', fontweight='bold')\n",
    "    ax2.set_ylabel('Understanding Degree V(s,s\\')', fontweight='bold')\n",
    "    ax2.set_title('Convergence Analysis', fontweight='bold', fontsize=12)\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    # 3. Non-commutativity Heatmap\n",
    "    ax3 = fig.add_subplot(gs[0, 2])\n",
    "    commutativity_matrix = np.array([[results['U_AB'], results['U_AB'] - results['U_BA']],\n",
    "                                   [results['U_BA'] - results['U_AB'], results['U_BA']]])\n",
    "    im = ax3.imshow(commutativity_matrix, cmap='RdBu_r', aspect='auto', vmin=-0.5, vmax=1.0)\n",
    "    ax3.set_xticks([0, 1])\n",
    "    ax3.set_yticks([0, 1])\n",
    "    ax3.set_xticklabels(['A→B', 'B→A'])\n",
    "    ax3.set_yticklabels(['A→B', 'B→A'])\n",
    "    ax3.set_title('Non-commutativity Matrix', fontweight='bold', fontsize=12)\n",
    "\n",
    "    # Add text annotations\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            text = ax3.text(j, i, f'{commutativity_matrix[i, j]:.3f}',\n",
    "                          ha=\"center\", va=\"center\", color=\"white\", fontweight='bold')\n",
    "\n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(im, ax=ax3, shrink=0.6)\n",
    "    cbar.set_label('Understanding', rotation=270, labelpad=15, fontweight='bold')\n",
    "\n",
    "    # 4. Understanding Distribution (Violin Plot)\n",
    "    ax4 = fig.add_subplot(gs[1, 0])\n",
    "    understanding_data = [v_ab_values, v_ba_values]\n",
    "    labels = ['V(A→B)', 'V(B→A)']\n",
    "\n",
    "    # Create violin plot with different approach for newer seaborn versions\n",
    "    parts = ax4.violinplot(understanding_data, positions=[1, 2], showmeans=True, showmedians=True)\n",
    "    ax4.set_xticks([1, 2])\n",
    "    ax4.set_xticklabels(labels)\n",
    "    ax4.set_ylabel('Understanding Degree', fontweight='bold')\n",
    "    ax4.set_title('Understanding Distribution', fontweight='bold', fontsize=12)\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "\n",
    "    # 5. Conversation Metrics Radar Chart\n",
    "    ax5 = fig.add_subplot(gs[1, 1], projection='polar')\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics = [\n",
    "        results['U_AB'],\n",
    "        results['U_BA'],\n",
    "        1.0 - abs(results['U_AB'] - results['U_BA']),   # Commutativity (1 - |difference|)\n",
    "        1.0 / (1.0 + results['iterations_AB']),         # Efficiency A→B\n",
    "        1.0 / (1.0 + results['iterations_BA']),         # Efficiency B→A\n",
    "        (results['U_AB'] + results['U_BA']) / 2         # Average Understanding\n",
    "    ]\n",
    "\n",
    "    categories = ['V(A→B)', 'V(B→A)', 'Commutativity', 'Efficiency AB', 'Efficiency BA', 'Avg Understanding']\n",
    "\n",
    "    # Number of variables\n",
    "    N = len(categories)\n",
    "\n",
    "    # Angle for each axis\n",
    "    angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "    angles += angles[:1]  # Complete the circle\n",
    "\n",
    "    # Add values and complete the circle\n",
    "    metrics += metrics[:1]\n",
    "\n",
    "    ax5.plot(angles, metrics, 'o-', linewidth=2, label='RC Metrics', color='#2E86AB')\n",
    "    ax5.fill(angles, metrics, alpha=0.25, color='#2E86AB')\n",
    "\n",
    "    # Add category labels\n",
    "    ax5.set_xticks(angles[:-1])\n",
    "    ax5.set_xticklabels(categories)\n",
    "    ax5.set_ylim(0, 1)\n",
    "    ax5.set_title('RC Understanding Metrics', fontweight='bold', fontsize=12, pad=20)\n",
    "    ax5.grid(True)\n",
    "\n",
    "    # 6. Iteration Efficiency Analysis\n",
    "    ax6 = fig.add_subplot(gs[1, 2])\n",
    "    iteration_data = {\n",
    "        'Direction': ['A→B', 'B→A'],\n",
    "        'Iterations': [results['iterations_AB'], results['iterations_BA']],\n",
    "        'Final Understanding': [results['U_AB'], results['U_BA']]\n",
    "    }\n",
    "\n",
    "    x = np.arange(len(iteration_data['Direction']))\n",
    "    width = 0.35\n",
    "\n",
    "    bars1 = ax6.bar(x - width/2, iteration_data['Iterations'], width,\n",
    "                   label='Iterations', color='#F18F01', alpha=0.7)\n",
    "\n",
    "    ax6_twin = ax6.twinx()\n",
    "    bars2 = ax6_twin.bar(x + width/2, iteration_data['Final Understanding'], width,\n",
    "                        label='Final Understanding', color='#2E86AB', alpha=0.7)\n",
    "\n",
    "    ax6.set_xlabel('Direction', fontweight='bold')\n",
    "    ax6.set_ylabel('Iterations', fontweight='bold', color='#F18F01')\n",
    "    ax6_twin.set_ylabel('Final Understanding', fontweight='bold', color='#2E86AB')\n",
    "    ax6.set_title('Iteration vs Understanding Efficiency', fontweight='bold', fontsize=12)\n",
    "    ax6.set_xticks(x)\n",
    "    ax6.set_xticklabels(iteration_data['Direction'])\n",
    "\n",
    "    # Add value labels\n",
    "    for bar, value in zip(bars1, iteration_data['Iterations']):\n",
    "        height = bar.get_height()\n",
    "        ax6.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "                f'{int(value)}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "    for bar, value in zip(bars2, iteration_data['Final Understanding']):\n",
    "        height = bar.get_height()\n",
    "        ax6_twin.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                     f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "    # Main title\n",
    "    fig.suptitle('RC Understanding Equation: Comprehensive Analysis',\n",
    "                fontsize=16, fontweight='bold', y=0.95)\n",
    "\n",
    "    # Use constrained_layout instead of tight_layout for better polar plot compatibility\n",
    "    plt.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(f\"{save_path}_comprehensive.png\", dpi=300, bbox_inches='tight')\n",
    "        print(f\"Comprehensive plot saved to {save_path}_comprehensive.png\")\n",
    "\n",
    "    if show_plots:\n",
    "        plt.show()\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0ba94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_convergence_analysis(results: dict, save_path: str = None, show_plot: bool = True):\n",
    "    \"\"\"\n",
    "    Create a detailed convergence analysis plot for RC Understanding Equation.\n",
    "\n",
    "    Args:\n",
    "        results: Dictionary containing experiment results\n",
    "        save_path: Optional path to save the plot\n",
    "        show_plot: Whether to display the plot\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "    # Extract convergence data\n",
    "    v_ab_evolution = []\n",
    "    v_ba_evolution = []\n",
    "\n",
    "    for conv_data in results['conversation_history_AB']:\n",
    "        if 'U' in conv_data and conv_data['U'] > 0:\n",
    "            v_ab_evolution.append(conv_data['U'])\n",
    "\n",
    "    for conv_data in results['conversation_history_BA']:\n",
    "        if 'U' in conv_data and conv_data['U'] > 0:\n",
    "            v_ba_evolution.append(conv_data['U'])\n",
    "\n",
    "    iterations_ab = list(range(1, len(v_ab_evolution) + 1))\n",
    "    iterations_ba = list(range(1, len(v_ba_evolution) + 1))\n",
    "\n",
    "    # Plot 1: Understanding Evolution\n",
    "    ax1.plot(iterations_ab, v_ab_evolution, 'o-', linewidth=3, markersize=8,\n",
    "             label='V(A→B)', color='#2E86AB', alpha=0.8)\n",
    "    ax1.plot(iterations_ba, v_ba_evolution, 's-', linewidth=3, markersize=8,\n",
    "             label='V(B→A)', color='#A23B72', alpha=0.8)\n",
    "\n",
    "    ax1.set_xlabel('Iteration', fontweight='bold', fontsize=12)\n",
    "    ax1.set_ylabel('Understanding Degree V(s,s\\')', fontweight='bold', fontsize=12)\n",
    "    ax1.set_title('Convergence of Understanding Degrees', fontweight='bold', fontsize=14)\n",
    "    ax1.legend(fontsize=11)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # Add convergence threshold\n",
    "    max_iter = max(len(v_ab_evolution), len(v_ba_evolution))\n",
    "    ax1.axhline(y=EPSILON, color='red', linestyle='--', alpha=0.7,\n",
    "                label=f'Convergence threshold ε = {EPSILON}')\n",
    "\n",
    "    # Plot 2: Delta V (change between iterations)\n",
    "    ax2.set_title('Convergence Rate Analysis', fontweight='bold', fontsize=14)\n",
    "\n",
    "    if len(v_ab_evolution) > 1:\n",
    "        delta_v_ab = [abs(v_ab_evolution[i] - v_ab_evolution[i-1]) for i in range(1, len(v_ab_evolution))]\n",
    "        ax2.plot(range(2, len(v_ab_evolution) + 1), delta_v_ab, 'o-',\n",
    "                linewidth=3, markersize=8, label='ΔV(A→B)', color='#2E86AB', alpha=0.8)\n",
    "\n",
    "    if len(v_ba_evolution) > 1:\n",
    "        delta_v_ba = [abs(v_ba_evolution[i] - v_ba_evolution[i-1]) for i in range(1, len(v_ba_evolution))]\n",
    "        ax2.plot(range(2, len(v_ba_evolution) + 1), delta_v_ba, 's-',\n",
    "                linewidth=3, markersize=8, label='ΔV(B→A)', color='#A23B72', alpha=0.8)\n",
    "\n",
    "    ax2.axhline(y=EPSILON, color='red', linestyle='--', alpha=0.7,\n",
    "                label=f'Convergence threshold ε = {EPSILON}')\n",
    "    ax2.set_xlabel('Iteration', fontweight='bold', fontsize=12)\n",
    "    ax2.set_ylabel('|ΔV|', fontweight='bold', fontsize=12)\n",
    "    ax2.legend(fontsize=11)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_yscale('log')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(f\"{save_path}_convergence.png\", dpi=300, bbox_inches='tight')\n",
    "        print(f\"Convergence plot saved to {save_path}_convergence.png\")\n",
    "\n",
    "    if show_plot:\n",
    "        plt.show()\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225e130d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_conversation_timeline(results: dict, phase: str = 'AB', save_path: str = None, show_plot: bool = True):\n",
    "    \"\"\"\n",
    "    Create a timeline visualization of the conversation evolution.\n",
    "\n",
    "    Args:\n",
    "        results: Dictionary containing experiment results\n",
    "        phase: 'AB' or 'BA' to specify which phase to visualize\n",
    "        save_path: Optional path to save the plot\n",
    "        show_plot: Whether to display the plot\n",
    "    \"\"\"\n",
    "    conversation_key = f'conversation_history_{phase}'\n",
    "    if conversation_key not in results:\n",
    "        print(f\"No conversation history found for phase {phase}\")\n",
    "        return None\n",
    "\n",
    "    conversation_history = results[conversation_key]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "    # Extract data for timeline\n",
    "    iterations = []\n",
    "    understandings = []\n",
    "    questions = []\n",
    "\n",
    "    for conv_data in conversation_history:\n",
    "        if conv_data['iteration'] > 0:  # Skip initial setup\n",
    "            iterations.append(conv_data['iteration'])\n",
    "            understandings.append(conv_data.get('U', 0))\n",
    "            questions.append(conv_data.get('question', 'No question')[:50] + '...')\n",
    "\n",
    "    # Create timeline plot\n",
    "    if iterations and understandings:\n",
    "        ax.plot(iterations, understandings, 'o-', linewidth=3, markersize=10,\n",
    "                color='#F18F01', alpha=0.8, label=f'U({phase[0]}→{phase[1]})')\n",
    "\n",
    "        # Add question annotations\n",
    "        for i, (iter_num, understanding, question) in enumerate(zip(iterations, understandings, questions)):\n",
    "            ax.annotate(f'Q{iter_num}: {question}',\n",
    "                       xy=(iter_num, understanding),\n",
    "                       xytext=(10, 20 + (i % 3) * 15),\n",
    "                       textcoords='offset points',\n",
    "                       bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='lightblue', alpha=0.7),\n",
    "                       arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0.1'),\n",
    "                       fontsize=9)\n",
    "\n",
    "    ax.set_xlabel('Iteration', fontweight='bold', fontsize=12)\n",
    "    ax.set_ylabel('Understanding', fontweight='bold', fontsize=12)\n",
    "    ax.set_title(f'Conversation Timeline - Phase {phase[0]}→{phase[1]}', fontweight='bold', fontsize=14)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(fontsize=11)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(f\"{save_path}_timeline_{phase}.png\", dpi=300, bbox_inches='tight')\n",
    "        print(f\"Timeline plot saved to {save_path}_timeline_{phase}.png\")\n",
    "\n",
    "    if show_plot:\n",
    "        plt.show()\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a5e841",
   "metadata": {},
   "source": [
    "## Plotting Functions for RC Understanding Equation Results\n",
    "\n",
    "The following cells contain comprehensive plotting functions to visualize the results of RC Understanding Equation experiments. These functions create multiple types of visualizations:\n",
    "\n",
    "1. **`plot_experiment_results()`** - Creates a 6-panel comprehensive dashboard\n",
    "2. **`plot_convergence_analysis()`** - Detailed convergence study with understanding evolution and rate analysis\n",
    "3. **`plot_conversation_timeline()`** - Timeline visualization of conversation evolution with question annotations\n",
    "\n",
    "### Recent Fix\n",
    "Fixed seaborn v0.14.0 deprecation warning by updating the `violinplot` call to use the `hue` parameter instead of passing `palette` directly. The fix ensures compatibility with newer versions of seaborn while maintaining the same visual output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0973562f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run_default_experiment(topic=\"common sense reasoning\",\n",
    "                                 gt_model=LLM_MODEL_A1,\n",
    "                                 interpreter_model=LLM_MODEL_A3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7e3110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results to a file\n",
    "import json\n",
    "\n",
    "# create new folder for experiment results if it doesn't exist\n",
    "path = f\"results-{results['Model A']}-{results['Model B']}-{time.time()}\"\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "with open(f\"{path}/experiment_results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4aa145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot detailed convergence analysis\n",
    "_ = plot_convergence_analysis(results, save_path=f\"{path}/rc_convergence_analysis\", show_plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039aeec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot conversation timeline for phase A→B\n",
    "_ = plot_conversation_timeline(results, phase='AB', save_path=f\"{path}/rc_timeline_analysis\", show_plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0896b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot conversation timeline for phase B→A\n",
    "_ = plot_conversation_timeline(results, phase='BA', save_path=f\"{path}/rc_timeline_analysis\", show_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671c5d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comprehensive experiment results with all statistical charts\n",
    "_ = plot_experiment_results(results, save_path=f\"{path}/rc_comprehensive_analysis\", show_plots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4f2155",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
