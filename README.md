# Recursive Consciousness: Modeling Minds in Forgetful Systems

*Or how the universe remembers itself through us.*

#### Author: Stan Miasnikov, April 2025

### Abstract

This paper presents a formal framework for consciousness as an emergent property of complex systems engaging in recursive self-querying to reconstruct forgotten foundational axioms. Integrating modal logic, category theory, and information theory, we model consciousness as a self-referential mechanism for self-discovery. Empirical validation via multi-agent simulations shows that agents, driven by stateless prompts, quickly form cooperative networks and interrogate their computational environment, though they face limitations due to their design and limited computational complexity. These behaviors, while not signifying consciousness, offer a computational parallel to the theorized recursive introspection. This framework provides a new lens on how systems might pursue self-understanding, enriching discussions on consciousness and self-awareness.

Full paper available at [Recursive Consciousness: Modeling Minds in Forgetful Systems](http://dx.doi.org/10.13140/RG.2.2.26969.22884).

Follow up paper - [The External Projection of Meaning in Recursive Consciousness](http://dx.doi.org/10.13140/RG.2.2.10988.27524)

### Abstract

The second paper extends the *Recursive Consciousness framework* by formalizing the external projection of meaning within a recursive hierarchy of **nested closed Gödelian systems** $ U_n, U_{n+1}, ... $ Each $U$ n is a closed formal system subject to Gödelian incompleteness, with $U_{n+1}$ containing $U_n$ as a subsystem. Authors observe that an agent (e.g., a subsystem $C_n$ within $U_n$) may achieve internal epistemic fixpoints (formally $\Box p \leftrightarrow p$ or $K_{C_n}p \leftrightarrow p$, yet the actual semantic content of propositions $p$ is not intrinsic to the agent. Instead, meanings are projected externally by a higher ontological layer (such as $U_{n+1}$) or by external interpreters (e.g., human supervisors in $U_1$ of AI agents in a simulated Universe $U_0$). The paper introduces functor $ M: \mathcal{C}_{\mathrm{out}} \to \mathcal{C}_{\mathrm{sem}} $ mapping agent outputs to semantic contents, distinct from the forgetful functor $F$ in the original model. Importantly, $M$ is not computable within $U_n$ or internally accessible to $C_n$; it depends on a higher-level interpreter's context.


[Debugger Agent Test](debugger/README.md) is a Jupyter notebook that implements a simple test of the recursive consciousness model for debugging code. It uses a combination of LLMs (Large Language Models) and structured data to analyze and improve code functions iteratively. The script is designed to be modular, allowing for easy integration with different LLMs and data sources.

[philosophical-ai-v8.ipynb](philosophical-ai-v8.ipynb) is a Jupyter notebook provides a platform to explore imagined machine self-awareness by observing how an AI engages in self-referential reasoning and achieves a form of understanding of its own processes.
